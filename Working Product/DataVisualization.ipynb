{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54d0275",
   "metadata": {},
   "source": [
    "# Pre Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4fd4c",
   "metadata": {},
   "source": [
    "These functions are responsible for generating the objects needed for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c738657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_per_topic_tuple_df(topic_level_sentiment, df_topics):\n",
    "    \"\"\"\n",
    "    This function takes in two previously generated objects; a dictionary of \n",
    "    articles that contain the sentiment analysis value for each topic, and \n",
    "    a dataframe of articles (rows) and topics (columns) where each cell is the \n",
    "    relevancy of the topic for that article.\n",
    "    \n",
    "    It combines the two objects into one dataframe of articles (rows) and topics \n",
    "    (columns) where each cell is a tuple (sentiment, relevancy) of that article for \n",
    "    that topic. Returns the tuple dataframe.\n",
    "    \"\"\"\n",
    "    # Turn to dataframe\n",
    "    topic_level_sentiment_df = dictionary_to_data_frame(topic_level_sentiment)\n",
    "    # Transpose rows and columns\n",
    "    topic_level_sentiment_df = topic_level_sentiment_df.transpose()\n",
    "\n",
    "    def to_int(str):\n",
    "        return int(str)\n",
    "    # Rename str names to ints\n",
    "    topic_level_sentiment_df =  topic_level_sentiment_df.rename(to_int,\n",
    "                                                                axis= 'columns')\n",
    "\n",
    "    # Zip the two dfs together\n",
    "    tuple_df = pd.concat([topic_level_sentiment_df, df_topics]).\\\n",
    "                        groupby(level=0).agg(tuple)\n",
    "    return tuple_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8a4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe_for_datavis(dataframe, lda_model, corpus):\n",
    "    \"\"\"\n",
    "    This function takes our generated sentiment analysis dataframe, and the LDA \n",
    "    model and corpus for the texts. It adds several variables to the dataframe \n",
    "    (Topics, Main Topic, Main Topic Score (relevancy), Shortened Address) and \n",
    "    then sorts them based on main topic for data visualization. Returns a new, \n",
    "    processed dataframe.\n",
    "    \"\"\"\n",
    "    # Make deep copy of dataframe to prevent changes to original\n",
    "    df = dataframe.copy(deep = True)\n",
    "\n",
    "    df['Topics'] = lda_model.get_document_topics(corpus)\n",
    "\n",
    "    sf = pd.DataFrame(data=df['Topics'])\n",
    "    af = pd.DataFrame()\n",
    "\n",
    "    df_topic_list = []\n",
    "    df_score_list = []\n",
    "\n",
    "    # Here we find most relevant topic for each article\n",
    "    for ind in sf.index:\n",
    "        rtl = sf['Topics'][ind]\n",
    "        relevant_topic = -1\n",
    "        relevant_topic_score = 0\n",
    "        for (topic,score) in rtl:\n",
    "            if score > relevant_topic_score:\n",
    "                relevant_topic = topic\n",
    "                relevant_topic_score = score\n",
    "        df_topic_list.append(relevant_topic)\n",
    "        df_score_list.append(relevant_topic_score)\n",
    "\n",
    "    # We add main topic and the main topic's relevancy score\n",
    "    # Add most relevant topic to df\n",
    "    df['Main Topic'] = df_topic_list\n",
    "    df['Main Topic Score'] = df_score_list\n",
    "\n",
    "    associated_words = []\n",
    "    for topic_id in df['Main Topic']:\n",
    "        associated_words.append([([word for word, _ in lda_model.show_topic(topic_id)])])\n",
    "    df['Associated Words'] = associated_words\n",
    "\n",
    "    # Grab list of urls for parsing\n",
    "    url_list = df['URL'].to_list()\n",
    "\n",
    "    # Iterate through urls and parse them into their base site names\n",
    "    for i, url in enumerate(url_list):\n",
    "        if 'https://' in url:\n",
    "            url = url.split(\"https://\")[1]\n",
    "            url_list[i] = url.split(\"/\")[0]\n",
    "        elif 'http://' in url:\n",
    "            url = url.split(\"http://\")[1]\n",
    "            url_list[i] = url.split(\"/\")[0]\n",
    "        else:\n",
    "            url_list[i] = url.split(\"/\")[0]\n",
    "\n",
    "    # Shorten the urls to make them easier to read in hover text\n",
    "    df['Shortened Address'] = url_list\n",
    "\n",
    "    # Sort df by main topic so it is in order in the graph.\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae90b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_relevancy_dataframe(lda_model):\n",
    "    \"\"\"\n",
    "    Taking in our LDA model, creates a new dataframe of all articles (rows) and \n",
    "    all topics (columns) where a cell corresponds to the relevancy of that \n",
    "    topic on that article. So cell [2,3] with a value of 0.988 means article 2 \n",
    "    has a 0.988 relevancy score for Topic 3.\n",
    "    \"\"\"\n",
    "    # Gather all topics per document as a list of lists of tuples\n",
    "    document_topics = [lda_model.get_document_topics \\\n",
    "                       (item, minimum_probability = 0.0) for item in corpus]\n",
    "\n",
    "    # Get the num of topics to add to df\n",
    "    topic_cols = [x[0] for x in document_topics[0]]\n",
    "\n",
    "    # Make df with topics\n",
    "    df_topics = pd.DataFrame(columns = topic_cols)\n",
    "\n",
    "    for i in document_topics:\n",
    "        topic_scores = [x[1] for x in i]\n",
    "        df_topics.loc[len(df_topics.index)] = topic_scores\n",
    "\n",
    "    # Document_topics\n",
    "    return df_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45642086",
   "metadata": {},
   "source": [
    "# Data Visualization Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ead48",
   "metadata": {},
   "source": [
    "Below contains our functions that generate data visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d665a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_all_articles_on_main_topic(df, plot_type='scatter'):\n",
    "    \"\"\"\n",
    "    Taking in our (pre-processed) main dataframe, generates either a scatter or box plot\n",
    "    of all articles sorted by their main topic and plotted along their sentiment values.\n",
    "    Also returns useful information through hovertext.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the article data.\n",
    "        plot_type (str): Type of plot ('scatter' or 'box'). Default is 'scatter'.\n",
    "    \"\"\"\n",
    "    # Define a common category order for the main topics\n",
    "    common_category_order = df['Main Topic'].unique()\n",
    "\n",
    "    if plot_type == 'scatter':\n",
    "        # Create a scatter plot with x, y, and color from our df.\n",
    "        fig = px.scatter(df, x=\"Main Topic\", y=\"Sentiment Score\",\n",
    "                         size=\"Main Topic Score\",\n",
    "                         custom_data=['ID',\n",
    "                                      'Article Title',\n",
    "                                      'Associated Words',\n",
    "                                      'Shortened Address',\n",
    "                                      'Sentiment Label',\n",
    "                                      'Main Topic Score'],\n",
    "                         title=\"Articles Sorted By Main Topic\")\n",
    "        # Set the y-axis range to make it symmetrical around zero\n",
    "        y_max = df['Sentiment Score'].abs().max()\n",
    "        fig.update_yaxes(range=[-y_max, y_max])\n",
    "            # Set the hover text to show what was in custom_data\n",
    "        fig.update_traces(hovertemplate=\"<br>\".join(\\\n",
    "                                [\"ID: %{customdata[0]}\",\n",
    "                                 \"Article Title: %{customdata[1]}\",\n",
    "                                 \"Associated Words: %{customdata[2]}\",\n",
    "                                 \"Address: %{customdata[3]}\",\n",
    "                                 \"Sentiment Label: %{customdata[4]}\",\n",
    "                                 \"Main Topic Score : %{customdata[5]}\"]))\n",
    "\n",
    "    elif plot_type == 'box':\n",
    "        # Create a box plot with main topic on the x-axis and sentiment score on the y-axis\n",
    "        fig = px.box(df, x=\"Main Topic\", y=\"Sentiment Score\",\n",
    "                     category_orders={\"Main Topic\": common_category_order},  # Set category order\n",
    "                     title=\"Articles Sorted By Main Topic\",\n",
    "                     labels={'Main Topic': 'Main Topic', 'Sentiment Score': 'Sentiment Score'})\n",
    "        # Set the y-axis range to make it symmetrical around zero\n",
    "        y_max = df['Sentiment Score'].abs().max()\n",
    "        fig.update_yaxes(range=[-y_max, y_max])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid plot_type. Choose 'scatter' or 'box'.\")\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c891a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_topic_cluster_tsne(lda_model, df):\n",
    "    \"\"\"\n",
    "    Taking in both our main (pre-processed) dataframe and our LDA model, \n",
    "    generates a topic clustering graph based on the topic relevancy of \n",
    "    each article's topic. Displays the clustering in a 2D space.\n",
    "    \"\"\"\n",
    "    num_topics = lda_model.num_topics\n",
    "\n",
    "    # Get Topic Weights\n",
    "    topic_weights = []\n",
    "    for i in df[\"Topics\"]:\n",
    "        per_doc_list = [None] * num_topics\n",
    "        #print(len(per_doc_list))\n",
    "        for x in i:\n",
    "            #print(x)\n",
    "            per_doc_list[x[0]] = x[1]\n",
    "        topic_weights.append(per_doc_list)\n",
    "\n",
    "    # Array of topic weights  \n",
    "    arr = pd.DataFrame(topic_weights).fillna(0).to_numpy()\n",
    "\n",
    "    # tSNE Model Creation\n",
    "    tsne_model = TSNE(n_components=2, verbose=1,\n",
    "                  random_state=0, angle=.99, \n",
    "                  init='pca', perplexity = (arr.shape[0] - 1) / 3)\n",
    "\n",
    "    tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "    # Formatting\n",
    "    # Turn them into ints so we can sort by main topic, then back to str\n",
    "    df['Main Topic'] = df['Main Topic'].apply(int)\n",
    "\n",
    "    # Sort by main topic to make the legend pretty\n",
    "    df = df.sort_values(by=['Main Topic'],ascending = True)\n",
    "\n",
    "    # This makes it so we can use main topic as categorical data\n",
    "    df['Main Topic'] = df['Main Topic'].apply(str)\n",
    "\n",
    "    # Creating the cluster graph in plotly\n",
    "    fig_cluster = px.scatter(df, x = tsne_lda[:,0],y = tsne_lda[:,1],\n",
    "                        custom_data = ['ID',\n",
    "                                       'Article Title',\n",
    "                                       'Associated Words',\n",
    "                                       'Shortened Address',\n",
    "                                       'Sentiment Label',\n",
    "                                       'Main Topic Score'],\n",
    "                        color = \"Main Topic\",\n",
    "                        size = \"Main Topic Score\",\n",
    "                        title = \"Topic Clustering Graph\",\n",
    "                        labels = dict(color = \"Main Topic\"))\n",
    "\n",
    "    # Set the hover text to show whatwas in custom_data\n",
    "    fig_cluster.update_traces(hovertemplate=\\\n",
    "                              \"<br>\".join([\"ID: %{customdata[0]}\",\n",
    "                                           \"Article Title: %{customdata[1]}\",\n",
    "                                           \"Associated Words: %{customdata[2]}\",\n",
    "                                           \"Address: %{customdata[3]}\",\n",
    "                                           \"Sentiment Label: %{customdata[4]}\",\n",
    "                                           \"Main Topic Score: %{customdata[5]}\"]))\n",
    "\n",
    "    fig_cluster.show()\n",
    "\n",
    "    # Turn back to int\n",
    "    df['Main Topic'] = df['Main Topic'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54220d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_topic_cluster_tsne_3d(lda_model, df):\n",
    "    \"\"\"\n",
    "    Taking in both our main (pre-processed) dataframe and our LDA model, \n",
    "    generates a topic clustering graph based on the topic relevancy of \n",
    "    each article's topic. Then applies each article's sentiment value score \n",
    "    to make a 3D visualization. X-axis and y-axis represents the topic space\n",
    "    and article clustering, while the z-axis is the article's sentiment score.\n",
    "    \"\"\"\n",
    "    num_topics = lda_model.num_topics\n",
    "\n",
    "    # Get Topic Weights\n",
    "    topic_weights = []\n",
    "    for i in df[\"Topics\"]:\n",
    "        per_doc_list = [None] * num_topics\n",
    "        #print(len(per_doc_list))\n",
    "        for x in i:\n",
    "            #print(x)\n",
    "            per_doc_list[x[0]] = x[1]\n",
    "        topic_weights.append(per_doc_list)\n",
    "\n",
    "    # Array of topic weights  \n",
    "    arr = pd.DataFrame(topic_weights).fillna(0).to_numpy()\n",
    "\n",
    "    # tSNE Model Creation\n",
    "    tsne_model = TSNE(n_components=2, verbose=1,\n",
    "                  random_state=0, angle=.99, \n",
    "                  init='pca', perplexity = (arr.shape[0] - 1) / 3)\n",
    "\n",
    "    tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "    # Formatting\n",
    "    # Turn them into ints so we can sort by main topic, then back to str\n",
    "    df['Main Topic'] = df['Main Topic'].apply(int)\n",
    "\n",
    "    # Sort by main topic to make the legend pretty\n",
    "    df = df.sort_values(by=['Main Topic'],ascending = True)\n",
    "\n",
    "    # This makes it so we can use main topic as categorical data\n",
    "    df['Main Topic'] = df['Main Topic'].apply(str)\n",
    "\n",
    "    # Creating the cluster graph in plotly\n",
    "    fig_cluster = px.scatter_3d(df, x = tsne_lda[:,0],\n",
    "                                y = tsne_lda[:,1],\n",
    "                                z = \"Sentiment Score\",\n",
    "                        custom_data = ['ID',\n",
    "                                       'Article Title',\n",
    "                                       'Associated Words',\n",
    "                                       'Shortened Address',\n",
    "                                       'Sentiment Label',\n",
    "                                       'Main Topic'],\n",
    "                        color = \"Main Topic\",\n",
    "                        size = \"Main Topic Score\",\n",
    "                        title = \"Topic Clustering Graph\",\n",
    "                        labels = dict(color = \"Main Topic\"))\n",
    "\n",
    "    # Set the hover text to show what was in custom_data\n",
    "    fig_cluster.update_traces(hovertemplate=\\\n",
    "                              \"<br>\".join([\"ID: %{customdata[0]}\",\n",
    "                                           \"Article Title: %{customdata[1]}\",\n",
    "                                           \"Associated Words: %{customdata[2]}\",\n",
    "                                           \"Address: %{customdata[3]}\",\n",
    "                                           \"Sentiment Label: %{customdata[4]}\",\n",
    "                                           \"Main Topic: %{customdata[5]}\"]))\n",
    "\n",
    "    fig_cluster.show()\n",
    "\n",
    "    # Turn back to int\n",
    "    df['Main Topic'] = df['Main Topic'].apply(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f8d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_single_topic_subjectivity_vs_sentiment(df, topic_num):\n",
    "    \"\"\"\n",
    "    Taking in the main (pre-processed) dataframe and an int corresponding to \n",
    "    the topic number you want to visualize, generates a 2D plot of all articles \n",
    "    with that topic number as their main topic, plotted along their \n",
    "    sentiment scores and their subjectivity scores (higher is more subjective).\n",
    "    \"\"\"\n",
    "    df_topic = df[df['Main Topic'] == topic_num]\n",
    "\n",
    "\n",
    "    t_string = \"Sentiment Analysis on Topic \" + str(topic_num)\n",
    "    fig = px.scatter(df_topic,\n",
    "                     x = \"Subjectivity Score\",\n",
    "                     y = \"Sentiment Score\",\n",
    "                     size = \"Main Topic Score\",\n",
    "                     hover_name = \"Shortened Address\",\n",
    "                     title = t_string,\n",
    "                     custom_data = ['ID',\n",
    "                                    'Article Title',\n",
    "                                    'Associated Words',\n",
    "                                    'Shortened Address',\n",
    "                                    'Sentiment Score',\n",
    "                                    'Subjectivity Score',\n",
    "                                    'Main Topic'])\n",
    "\n",
    "    # Set the hover text to show what was in custom_data\n",
    "    fig.update_traces(hovertemplate=\\\n",
    "                              \"<br>\".join([\"ID: %{customdata[0]}\",\n",
    "                                           \"Article Title: %{customdata[1]}\",\n",
    "                                           \"Associated Words: %{customdata[2]}\",\n",
    "                                           \"Address: %{customdata[3]}\",\n",
    "                                           \"Sentiment Score: %{customdata[4]}\",\n",
    "                                           \"Subjectivity Score: %{customdata[5]}\",\n",
    "                                           \"Main Topic: %{customdata[6]}\"]))\n",
    "                              \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83574bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_topic_word_cloud(lda_model, topic_id):\n",
    "    \"\"\"\n",
    "    Taking in the lda model as well as the topic we want to visualize, creates a word cloud of the\n",
    "    top 10 words in that topic. The words are weighted (by size) by their relevance to the topic.\n",
    "    \"\"\"\n",
    "    # Initialize the WordCloud generator\n",
    "    cloud = WordCloud(background_color='white',\n",
    "                    width=2500,\n",
    "                    height=1800,\n",
    "                    max_words=10,\n",
    "                    colormap='tab10',  # You can choose a different colormap\n",
    "                    prefer_horizontal=1.0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))  # Create a single plot\n",
    "    topic_words = dict(lda_model.show_topic(topic_id, topn=10))  # Get the top words for the topic\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    ax.imshow(cloud)\n",
    "    ax.set_title('Topic ' + str(topic_id), fontdict=dict(size=16))\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12e40a",
   "metadata": {},
   "source": [
    "# K-means clustering and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe1e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_optimal_cluster_count(df_topics, max_clusters, pca_components):\n",
    "    \"\"\"\n",
    "    Taking in our topic relevancy dataframe, the max cluster count that you \n",
    "    want to stop at, and the number of PCA components to use, generates a \n",
    "    visualization of the inertia score of our clustering model for every number \n",
    "    of clusters up to the max.\n",
    "    \"\"\"\n",
    "    wcss = []\n",
    "\n",
    "    # Set to number of components\n",
    "    pca = PCA(pca_components)\n",
    "    # Apply principled component analysis\n",
    "    data = pca.fit_transform(df_topics)\n",
    "\n",
    "    for i in range(2, max_clusters):\n",
    "        model = KMeans(n_clusters = i, init = \"k-means++\", n_init = 10)\n",
    "        model.fit(data)\n",
    "        wcss.append(model.inertia_)\n",
    "\n",
    "    # Plot inertia for the different number of clusters\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(2, max_clusters), wcss)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d56249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kmeans_clustering(dataframe, df_topics, num_clusters, pca_components):\n",
    "    \"\"\"\n",
    "    Taking in our topic relevancy dataframe, the number of clusters, and the \n",
    "    number of PCA components, generates a k-means clustering using those \n",
    "    parameters and visualizes it in a 2D space. The clustering is based solely \n",
    "    on the topic relevancy for each article. Also generates centers of each \n",
    "    cluster shown with an 'X'.\n",
    "    \"\"\"\n",
    "    # Make deep copy of our dataframe to add new temp variables\n",
    "    main_df = dataframe.copy(deep=True)\n",
    "    # Sort by ID to make sure the 2 dfs are aligned when we merge them\n",
    "    main_df = main_df.sort_values('ID')\n",
    "\n",
    "    # Dimension reduction to [pca_components] variables\n",
    "    pca = PCA(pca_components)\n",
    "\n",
    "    # Apply principled component analysis to topic relevancy\n",
    "    data = pca.fit_transform(df_topics)\n",
    "\n",
    "    # Creating KMeans model\n",
    "    model = KMeans(n_clusters = num_clusters, init = \"k-means++\", n_init = 10)\n",
    "    kmeans = model.fit_predict(data)\n",
    "    centers = np.array(model.cluster_centers_)\n",
    "\n",
    "    # Transform into dataframe to add to our main dataframe\n",
    "    data_df = pd.DataFrame({'x': data[:,0], 'y': data[:,1]})\n",
    "\n",
    "    # Gather the cluster information for the kmeans clusters\n",
    "    data_df['Cluster'] = kmeans\n",
    "\n",
    "    # Getting columns to add to main dataframe\n",
    "    x_col = data_df['x']\n",
    "    y_col = data_df['y']\n",
    "    cluster_col = data_df['Cluster']\n",
    "\n",
    "    # Add clustering data to our main dataframe\n",
    "    main_df = main_df.join(x_col)\n",
    "    main_df = main_df.join(y_col)\n",
    "    main_df = main_df.join(cluster_col)\n",
    "    \n",
    "    # Sort by cluster to make the legend pretty\n",
    "    main_df = main_df.sort_values(by=['Cluster'],ascending = True)\n",
    "    \n",
    "    # This makes it so we can use cluster as categorical data (for the purpose of making a legend)\n",
    "    main_df['Cluster'] = main_df['Cluster'].apply(str)\n",
    "\n",
    "    # Generate our scatter plot\n",
    "    fig = px.scatter(main_df, x = 'x',\n",
    "                     y = 'y',\n",
    "                     color = 'Cluster',\n",
    "                     labels = dict(color = \"Cluster\"),\n",
    "                     custom_data = ['ID',\n",
    "                                    'Article Title',\n",
    "                                    'Associated Words',\n",
    "                                    'Shortened Address',\n",
    "                                    'Sentiment Label',\n",
    "                                    'Main Topic'],)\n",
    "\n",
    "    # Add hover text\n",
    "    fig.update_traces(hovertemplate=\\\n",
    "                              \"<br>\".join([\"ID: %{customdata[0]}\",\n",
    "                                           \"Article Title: %{customdata[1]}\",\n",
    "                                           \"Associated Words: %{customdata[2]}\",\n",
    "                                           \"Address: %{customdata[3]}\",\n",
    "                                           \"Sentiment Label: %{customdata[4]}\",\n",
    "                                           \"Main Topic: %{customdata[5]}\"]))\n",
    "\n",
    "\n",
    "    # Turn cluster variable back into int\n",
    "    main_df['Cluster'] = main_df['Cluster'].apply(int)\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d1ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kmeans_clustering_3d(dataframe, df_topics, num_clusters, pca_components):\n",
    "    \"\"\"\n",
    "    Taking in our main article dataframe, topic relevancy dataframe, the number of clusters, and the \n",
    "    number of PCA components, generates a k-means clustering of articles by topic using those \n",
    "    parameters, and visualizes it in a 3D space. The clustering is based solely \n",
    "    on the topic relevancy for each article (colors each article by cluster). Then applies a z-axis\n",
    "    representing the sentiment value for each article. Produces a 3D scatter plot from this.\n",
    "    \"\"\"\n",
    "    # Make deep copy of our dataframe to add new temp variables\n",
    "    main_df = dataframe.copy(deep=True)\n",
    "    # Sort by ID to make sure the 2 dfs are aligned when we merge them\n",
    "    main_df = main_df.sort_values('ID')\n",
    "\n",
    "    # Dimension reduction to [pca_components] variables\n",
    "    pca = PCA(pca_components)\n",
    "\n",
    "    # Apply principled component analysis to topic relevancy\n",
    "    data = pca.fit_transform(df_topics)\n",
    "\n",
    "    # Creating KMeans model\n",
    "    model = KMeans(n_clusters = num_clusters, init = \"k-means++\", n_init = 10)\n",
    "    kmeans = model.fit_predict(data)\n",
    "    centers = np.array(model.cluster_centers_)\n",
    "\n",
    "    # Transform into dataframe to add to our main dataframe\n",
    "    data_df = pd.DataFrame({'x': data[:,0], 'y': data[:,1]})\n",
    "\n",
    "    # Gather the cluster information for the kmeans clusters\n",
    "    data_df['Cluster'] = kmeans\n",
    "\n",
    "    # Getting columns to add to main dataframe\n",
    "    x_col = data_df['x']\n",
    "    y_col = data_df['y']\n",
    "    cluster_col = data_df['Cluster']\n",
    "\n",
    "    # Add clustering data to our main dataframe\n",
    "    main_df = main_df.join(x_col)\n",
    "    main_df = main_df.join(y_col)\n",
    "    main_df = main_df.join(cluster_col)\n",
    "    \n",
    "    # Sort by cluster to make the legend pretty\n",
    "    main_df = main_df.sort_values(by=['Cluster'],ascending = True)\n",
    "    \n",
    "    # This makes it so we can use cluster as categorical data (to make a legend)\n",
    "    main_df['Cluster'] = main_df['Cluster'].apply(str)\n",
    "\n",
    "    # Generate our scatter plot\n",
    "    fig = px.scatter_3d(main_df, x = 'x',\n",
    "                     y = 'y',\n",
    "                     z = 'Sentiment Score',\n",
    "                     color = 'Cluster',\n",
    "                     labels = {\"Cluster\" : \"Cluster\"},\n",
    "                     custom_data = ['ID',\n",
    "                                    'Article Title',\n",
    "                                    'Associated Words',\n",
    "                                    'Shortened Address',\n",
    "                                    'Sentiment Label',\n",
    "                                    'Main Topic'],)\n",
    "\n",
    "    # Add hover text\n",
    "    fig.update_traces(hovertemplate=\\\n",
    "                              \"<br>\".join([\"ID: %{customdata[0]}\",\n",
    "                                           \"Article Title: %{customdata[1]}\",\n",
    "                                           \"Associated Words: %{customdata[2]}\",\n",
    "                                           \"Address: %{customdata[3]}\",\n",
    "                                           \"Sentiment Label: %{customdata[4]}\",\n",
    "                                           \"Main Topic: %{customdata[5]}\"]))\n",
    "\n",
    "    # Turn cluster variable back into int\n",
    "    main_df['Cluster'] = main_df['Cluster'].apply(int)\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9aaa7",
   "metadata": {},
   "source": [
    "# Sentiment X Topic Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68588ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_tuples_in_dataframe(input_df):\n",
    "    \"\"\"\n",
    "    Multiply the elements of tuples in the DataFrame to create a new DataFrame of float values.\n",
    "\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): The input DataFrame containing tuples.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with float values resulting from multiplying the tuples.\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame with the same structure as the input DataFrame\n",
    "    result_df = pd.DataFrame(index=input_df.index, columns=input_df.columns)\n",
    "\n",
    "    for column in input_df.columns:\n",
    "        # Use apply to multiply the elements of each tuple and store the result in the new DataFrame\n",
    "        result_df[column] = input_df[column].apply(lambda x: x[0] * x[1])\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_relevance(data):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of sentiment values against relevance values.\n",
    "\n",
    "    Parameters:\n",
    "    data (list of tuples): A list of tuples, where each tuple contains\n",
    "    sentiment and relevance values.\n",
    "        For example: [(0.5, 0.8), (0.3, 0.6), (-0.2, 0.4)]\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function takes a list of tuples, where each tuple represents a data point with\n",
    "    sentiment and relevance values. It then creates a scatter plot to visualize the\n",
    "    relationship between sentiment and relevance.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> data = [(0.5, 0.8), (0.3, 0.6), (-0.2, 0.4)]\n",
    "    >>> plot_sentiment_relevance(data)\n",
    "\n",
    "    This will display a scatter plot with sentiment values on the x-axis and relevance \n",
    "    values on the y-axis, along with labels and a title for the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the sentiment and relevance values into separate lists\n",
    "    sentiments, relevances = zip(*data)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(sentiments, relevances, c='blue', marker='o', alpha=0.5)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.title('Sentiment vs Relevance')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Relevance')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_reduction(doc_topic_matrix):\n",
    "    \"\"\"\n",
    "    Perform dimension reduction on a document-topic matrix using Principal Component Analysis (PCA).\n",
    "\n",
    "    Parameters:\n",
    "    doc_topic_matrix (numpy.ndarray): A matrix where each column represents a topic and each row \n",
    "    represents a document.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A reduced-dimensional topic space representation of the input matrix.\n",
    "\n",
    "    This function uses Principal Component Analysis (PCA) to reduce the dimensionality of the \n",
    "    document-topic matrix.\n",
    "    It returns a new topic space with a reduced number of dimensions.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> reduced_topic_space = dimension_reduction(doc_topic_matrix)\n",
    "    \"\"\"\n",
    "    # Number of dimensions for topic space\n",
    "    n_components = 2 \n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    topic_space = pca.fit_transform(doc_topic_matrix.T)\n",
    "    return topic_space\n",
    "\n",
    "def plot_pca(topic_space):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of points in a reduced-dimensional topic space.\n",
    "\n",
    "    Parameters:\n",
    "    topic_space (numpy.ndarray): A reduced-dimensional topic space representation.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function creates a scatter plot of points in a reduced-dimensional topic space, \n",
    "    providing a visualization of the relationships between topics or documents in a \n",
    "    lower-dimensional space.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> plot_pca(reduced_topic_space)\n",
    "    \"\"\"\n",
    "\n",
    "    topic_labels = [str(i) for i in range(len(topic_space))]\n",
    "\n",
    "    # Create a scatterplot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(*zip(*topic_space), marker='o', s=100, c='b')\n",
    "\n",
    "    # Add labels for each point\n",
    "\n",
    "    for i, (x, y) in enumerate(topic_space):\n",
    "        plt.annotate(topic_labels[i], (x, y), \n",
    "                     textcoords=\"offset points\",\n",
    "                     xytext=(0, 10), ha='center')\n",
    "\n",
    "    plt.title(\"Intertopic Distance Map\")\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7787663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_documents(doc_topic_matrix, topic_space, df_main):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of documents in a reduced-dimensional space, with \n",
    "    color-coded sentiment scores.\n",
    "\n",
    "    Parameters:\n",
    "    doc_topic_matrix (numpy.ndarray): A matrix where each column represents a topic and \n",
    "    each row represents a document.\n",
    "    topic_space (numpy.ndarray): A reduced-dimensional topic space representation.\n",
    "    df_main (pandas.DataFrame): A DataFrame containing document information, including \n",
    "    sentiment scores.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function takes a document-topic matrix, a reduced-dimensional topic space, \n",
    "    and a DataFrame with document information.\n",
    "    It calculates the coordinates for each document in the topic space, color-codes them \n",
    "    based on sentiment scores, and creates a scatter plot to visualize the relationship \n",
    "    between documents and topics.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> plot_documents(doc_topic_matrix, reduced_topic_space, df_main)\n",
    "    \"\"\"\n",
    "\n",
    "    colors = []\n",
    "    # Calculate the coordinates for each document\n",
    "    n_docs, n_topics = doc_topic_matrix.shape\n",
    "    document_space = []\n",
    "    for doc_id in range(n_docs):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        for topic_id in range(n_topics):\n",
    "            # Multiply the coordinates of the topic by the relavence of the topic\n",
    "            x += topic_space[topic_id][0] * doc_topic_matrix[topic_id][doc_id] \n",
    "            y += topic_space[topic_id][1] * doc_topic_matrix[topic_id][doc_id]\n",
    "        document_space.append((x,y))\n",
    "\n",
    "    colormap = mcolors.LinearSegmentedColormap.from_list('rg',[\"r\", \"w\", \"g\"], N=256)\n",
    "    for sentiment in df_main[\"Sentiment Score\"]:\n",
    "        # Normalize sentiment score to the range [0, 1]\n",
    "        normalized_sentiment = (sentiment + 0.2) / 0.4  # Map -0.2 to 0.2 to [0, 1]\n",
    "        sentiment_color = colormap(normalized_sentiment, alpha=1.0)\n",
    "        colors.append(sentiment_color)\n",
    "\n",
    "\n",
    "\n",
    "    x_coords, y_coords = zip(*document_space)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    \n",
    "    plt.scatter(x_coords, y_coords, marker='o', label='Documents', c=colors)\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.title('Scatter Plot of Data Points')\n",
    "\n",
    "    # Plot topics\n",
    "    x_coords_topics = [x for x, _ in topic_space]\n",
    "    y_coords_topics = [y for _, y in topic_space]\n",
    "    plt.scatter(x_coords_topics, y_coords_topics, color='black', marker='o', s=5, label='Topics')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_documents(df, topic_space):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of every topic in every docuement in a reduced-dimensional\n",
    "    topic space with color-coded sentiment scores.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): A DataFrame containing document information, \n",
    "    including topic relevance and sentiment scores.\n",
    "    topic_space (numpy.ndarray): A reduced-dimensional topic space representation.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function takes a DataFrame with document information, including topic relevance \n",
    "    and sentiment scores, and a reduced-dimensional topic space. It calculates the \n",
    "    coordinates for each document in the topic space, filters based on topic relevance,\n",
    "    and color-codes the points based on sentiment scores. It then creates a scatter plot to\n",
    "    visualize the relationship between documents and topics. The further from the center \n",
    "    the points are, the more relavent they are, while the closer they are, the more abstract \n",
    "    the documents are.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> plot_topic_documents(df, reduced_topic_space)\n",
    "    \"\"\"\n",
    "\n",
    "    colormap = mcolors.LinearSegmentedColormap.from_list('rg',[\"r\", \"w\", \"g\"], N=256)\n",
    "    colors = []\n",
    "\n",
    "    # Create separate DataFrames for the first and second values\n",
    "    doc_topic_matrix = pd.DataFrame()\n",
    "    sentiment_matrix = pd.DataFrame()\n",
    "\n",
    "    for column in df.columns:\n",
    "        doc_topic_matrix[column] = df[column].apply(lambda x: x[0])\n",
    "        sentiment_matrix[column] = df[column].apply(lambda x: x[1])\n",
    "\n",
    "    # Calculate the coordinates for each document\n",
    "    n_docs, n_topics = doc_topic_matrix.shape\n",
    "    document_topic_space = []\n",
    "    for doc_id in range(n_docs):\n",
    "        for topic_id in range(n_topics):\n",
    "            if doc_topic_matrix[topic_id][doc_id] > 0.1:\n",
    "                # Multiply the coordinates of the topic by the relavence of the topic\n",
    "                x = topic_space[topic_id][0] * doc_topic_matrix[topic_id][doc_id] \n",
    "                y = topic_space[topic_id][1] * doc_topic_matrix[topic_id][doc_id]\n",
    "                document_topic_space.append((x,y))\n",
    "                # Get color of point based on the sentiment\n",
    "                sentiment_color = colormap(sentiment_matrix[topic_id][doc_id])\n",
    "                colors.append(sentiment_color)\n",
    "\n",
    "\n",
    "    x_coords, y_coords = zip(*document_topic_space)\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.scatter(x_coords, y_coords, marker='o', label='Data Points', c=colors)\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.title('Scatter Plot of Data Points')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44177443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_heatmap(doc_topic_matrix, topic_space):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of documents in a reduced-dimensional topic space,\n",
    "    with color-coded sentiment scores.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): A DataFrame containing document information, including \n",
    "    topic relevance and sentiment scores.\n",
    "    topic_space (numpy.ndarray): A reduced-dimensional topic space representation.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function takes a DataFrame with document information, including topic\n",
    "    relevance and sentiment scores, and a reduced-dimensional topic space. It \n",
    "    calculates the coordinates for each document in the topic space, filters based\n",
    "    on topic relevance, and color-codes the points based on sentiment scores. \n",
    "    It then creates a scatter plot to visualize the Density of documents in the topic space.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> plot_topic_documents(df, reduced_topic_space)\n",
    "    \"\"\"\n",
    "    # Calculate the coordinates for each document\n",
    "    n_docs, n_topics = doc_topic_matrix.shape\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "\n",
    "    for doc_id in range(n_docs):\n",
    "        x = 0\n",
    "        y = 0\n",
    "\n",
    "        for topic_id in range(n_topics):\n",
    "            # Multiply the coordinates of the topic by the relevance of the topic\n",
    "            x += topic_space[topic_id][0] * doc_topic_matrix[topic_id][doc_id]\n",
    "            y += topic_space[topic_id][1] * doc_topic_matrix[topic_id][doc_id]\n",
    "\n",
    "        x_coords.append(x)\n",
    "        y_coords.append(y)\n",
    "\n",
    "    # Create a 2D grid for the heatmap\n",
    "    x_min, x_max = -max(x_coords), max(x_coords)\n",
    "    y_min, y_max = -max(y_coords), max(y_coords)\n",
    "\n",
    "    resolution = 2\n",
    "    x_range = int((x_max - x_min) * resolution) + 1\n",
    "    y_range = int((y_max - y_min) * resolution) + 1\n",
    "\n",
    "    heatmap = [[0 for _ in range(x_range + 1)] for _ in range(y_range + 1)]\n",
    "\n",
    "    for x, y in zip(x_coords, y_coords):\n",
    "        x_idx = int((x * resolution) + x_range/2)\n",
    "        y_idx = int((y * resolution) + y_range/2)\n",
    "        heatmap[y_idx][x_idx] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = [\"white\", \"red\"]\n",
    "    colormap = mcolors.LinearSegmentedColormap.from_list(\"custom\", colors, N=256)\n",
    "    plt.imshow(heatmap, cmap=colormap, extent=[x_min, x_max, y_min, y_max], origin='lower')\n",
    "    plt.colorbar(label='Topic_Density')\n",
    "\n",
    "    # Plot topics\n",
    "    x_coords_topics = [x for x, _ in topic_space]\n",
    "    y_coords_topics = [y for _, y in topic_space]\n",
    "    plt.scatter(x_coords_topics,\n",
    "                y_coords_topics, \n",
    "                color='black', \n",
    "                marker='o', \n",
    "                s=10, \n",
    "                label='Data Points')\n",
    "\n",
    "\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.title('Topic Heatmap')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eefc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_heatmap(doc_topic_matrix, topic_space, df_main):\n",
    "    \"\"\"\n",
    "    Create a heatmap of sentiment scores in a reduced-dimensional topic space.\n",
    "\n",
    "    Parameters:\n",
    "    doc_topic_matrix (numpy.ndarray): A matrix where each column represents a topic and each row represents a document.\n",
    "    topic_space (numpy.ndarray): A reduced-dimensional topic space representation.\n",
    "    df_main (pandas.DataFrame): A DataFrame containing document information, including sentiment scores.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function takes a document-topic matrix, a reduced-dimensional topic space, and a DataFrame with document\n",
    "    information, including sentiment scores. It calculates the coordinates for each document in the topic space and\n",
    "    creates a heatmap of sentiment scores in that space.\n",
    "\n",
    "    Example Usage:\n",
    "    >>> plot_sentiment_heatmap(doc_topic_matrix, reduced_topic_space, df_main)\n",
    "    \"\"\"\n",
    "    # Calculate the coordinates for each document\n",
    "    n_docs, n_topics = doc_topic_matrix.shape\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    sentiment_scores = df_main[\"Sentiment Score\"]\n",
    "\n",
    "    for doc_id in range(n_docs):\n",
    "        x = 0\n",
    "        y = 0\n",
    "\n",
    "        for topic_id in range(n_topics):\n",
    "            # Multiply the coordinates of the topic by the relevance of the topic\n",
    "            x += topic_space[topic_id][0] * doc_topic_matrix[topic_id][doc_id]\n",
    "            y += topic_space[topic_id][1] * doc_topic_matrix[topic_id][doc_id]\n",
    "\n",
    "        x_coords.append(x)\n",
    "        y_coords.append(y)\n",
    "\n",
    "    # Create a 2D grid for the heatmap\n",
    "    x_min, x_max = -max(x_coords), max(x_coords)\n",
    "    y_min, y_max = -max(y_coords), max(y_coords)\n",
    "\n",
    "    resolution = 2\n",
    "    x_range = int((x_max - x_min) * resolution) + 1\n",
    "    y_range = int((y_max - y_min) * resolution) + 1\n",
    "\n",
    "    heatmap = [[0 for _ in range(x_range + 1)] for _ in range(y_range + 1)]\n",
    "    heatmap_normalization_factor = [[0 for _ in range(x_range + 1)] for _ in range(y_range + 1)]\n",
    "\n",
    "    for x, y, sentiment in zip(x_coords, y_coords, sentiment_scores):\n",
    "        x_idx = int((x * resolution) + x_range/2)\n",
    "        y_idx = int((y * resolution) + y_range/2)\n",
    "        heatmap[y_idx][x_idx] += sentiment\n",
    "        heatmap_normalization_factor[y_idx][x_idx] += 1\n",
    "    \n",
    "    # Normalize the values to get an average sentiment\n",
    "    for y_idx in range(len(heatmap)):\n",
    "        for x_idx in range(len(heatmap[y_idx])):\n",
    "            if heatmap_normalization_factor[y_idx][x_idx] != 0:\n",
    "                heatmap[y_idx][x_idx] /= heatmap_normalization_factor[y_idx][x_idx]\n",
    "            \n",
    "\n",
    "    # Specify the colors for -1 (red), 0 (white), and 1 (green)\n",
    "    colors = [\"red\", \"white\", \"green\"]\n",
    "    colormap = mcolors.LinearSegmentedColormap.from_list(\"custom\", colors, N=256)\n",
    "\n",
    "    sentiment_max = max(abs(min(sentiment_scores)), max(sentiment_scores))\n",
    "\n",
    "    # Create a heatmap using the custom colormap\n",
    "    plt.imshow(heatmap, cmap=colormap, extent=[x_min, x_max, y_min, y_max], vmin=-sentiment_max, vmax=sentiment_max, origin='lower', aspect='auto')\n",
    "    plt.colorbar(label='Sentiment Score')\n",
    "\n",
    "    # Plot topics\n",
    "    x_coords_topics = [x for x, _ in topic_space]\n",
    "    y_coords_topics = [y for _, y in topic_space]\n",
    "    plt.scatter(x_coords_topics, y_coords_topics, color='black', marker='o', s=10, label='Data Points')\n",
    "\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.title('Sentiment Heatmap')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "434hw",
   "language": "python",
   "name": "434hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
