{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0139d3de",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "This file is used for performing sentiment analysis, both on an article as a whole, and on sentences that contain the main topic(s) related to that article for topic-based sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248255e",
   "metadata": {},
   "source": [
    "# Main Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f5cd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline_sentiment_analysis(urls):\n",
    "    \"\"\"\n",
    "    This is the main sentiment analysis pipeline function that is responsible \n",
    "    for first creating our sentiment analysis dataframe. Taking in a list of\n",
    "    URLs, it iterates through each of them, webscrapes the article text from the \n",
    "    URL, and then performs sentiment analysis on that text. As it iterates, the \n",
    "    data is being added to our sentimentDic dictionary. After we scrape and\n",
    "    analyze all URLs in the list, we turn it into a dataframe, remove all the\n",
    "    rows that failed webscraping, and return the dataframe.\n",
    "    \"\"\"\n",
    "    # Loops through our URLS and scraps the data\n",
    "    # Put all empty dictionaries here\n",
    "    sentiment_dic = {}\n",
    "\n",
    "    for count, x in enumerate(urls):\n",
    "        # Layman's way of showing progress\n",
    "        if count % 10 == 0:\n",
    "            print(str(count), \" Articles Completed\")\n",
    "\n",
    "        # The url of the article we want to webscrape and analyze\n",
    "        url = x\n",
    "\n",
    "        # Send the URL to get scraped, returning the text of the article\n",
    "        page_text, article_title = scrape_data(x)\n",
    "\n",
    "        # Runs sentiment analysis. Will need to make a new function and\n",
    "        # a new dictionary for each type of analysis we want to run.\n",
    "        # Will pass in the page_text, the dic, and x (the url)\n",
    "        sentiment_dic = sentiment_analysis(page_text,\n",
    "                                           sentiment_dic,\n",
    "                                           url,\n",
    "                                           article_title)\n",
    "\n",
    "\n",
    "    # For each analysis we run we need to then convert that dictionary\n",
    "    # with the following method\n",
    "    #print(len(sentiment_dic['URL']))\n",
    "    #print(len(sentiment_dic['Article Title']))\n",
    "    df = dictionary_to_data_frame(sentiment_dic)\n",
    "\n",
    "    # Clean dataframe by dropping all rows that failed webscraping\n",
    "    df = drop_failed_webscraping_rows(df)\n",
    "\n",
    "    # Reset all indices of the dataframe after removing failed rows\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    # Add unique ID to all articles in dataframe\n",
    "    df['ID'] = df.index\n",
    "    \n",
    "    # Move ID to the first column\n",
    "    column_to_move = df.pop(\"ID\")\n",
    "    df.insert(0, \"ID\", column_to_move)\n",
    "\n",
    "    percent_of_file_scraped = round((len(df.index) / count) * 100, 2)\n",
    "\n",
    "    per = '%'\n",
    "\n",
    "    print(str(percent_of_file_scraped) + per + \" of the csv was scraped.\")\n",
    "\n",
    "    average_sentiment = round(df['Sentiment Score'].mean(), 3)\n",
    "\n",
    "    average_subjectivity = round(df['Subjectivity Score'].mean(), 3)\n",
    "\n",
    "    print(\"The average sentiment score was: \" + str(average_sentiment))\n",
    "    print(\"The average subjectivity score was: \" + str(average_subjectivity))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab46cb1",
   "metadata": {},
   "source": [
    "# Article Level Sentiment Analysis\n",
    "This function is used to perform sentiment analysis on a single document text. Analysis is done with TextBlob with the help of Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea50e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text, dictionary, url, article_title):\n",
    "    \"\"\"\n",
    "    The main function for sentiment analysis of a single article. Takes in the \n",
    "    scraped article text, the sentiment analysis dictionary to add the data to,\n",
    "    and the url of the article. We add the url to our dictionary, then perform\n",
    "    sentiment analysis on the text by gathering the polarity (sentiment) and\n",
    "    subjectivity scores of the text. We label the sentiment score\n",
    "    Positive-Neutral-Negative and in-between, and then add all of the necessary\n",
    "    data we generated to the dictionary. We then create lists of positive words\n",
    "    and negative words, adding those to the dictionary as well. At the bottom,\n",
    "    if the web scraping failed for this article, we add results to the\n",
    "    dictionary that signal there was an error. Finally we return the dictionary\n",
    "    with its new entry.\n",
    "    \"\"\"\n",
    "    if len(dictionary) == 0:\n",
    "        dictionary = {\n",
    "            \"URL\": [],\n",
    "            \"Article Title\": [],\n",
    "            \"Sentiment Score\": [],\n",
    "            \"Sentiment Label\": [],\n",
    "            \"Subjectivity Score\": [],\n",
    "            \"Positive Words\": [],\n",
    "            \"Negative Words\": [],\n",
    "            \"Text\": []\n",
    "            }\n",
    "\n",
    "    # If there was an error while parsing the document we will not do any sentiment analysis\n",
    "    # on the article text.\n",
    "    if text[0:8] != \"PARERROR\":\n",
    "        # Start the sentiment analysis now\n",
    "        dictionary[\"URL\"].append(url)\n",
    "        dictionary[\"Article Title\"].append(article_title)\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Get's sentiment and subjectivity\n",
    "        sentiment = doc._.blob.polarity\n",
    "        sentiment = round(sentiment,2)\n",
    "        subjectivity = doc._.blob.subjectivity\n",
    "        subjectivity = round(subjectivity,2)\n",
    "\n",
    "        # Gives positive or negative label\n",
    "        if sentiment >= -0.25 and sentiment <= 0.25:\n",
    "            sent_label = \"Neutral\"\n",
    "        elif sentiment > 0.25 and sentiment < 0.5:\n",
    "            sent_label = \"Neutral Positive\"\n",
    "        elif sentiment >= 0.5:\n",
    "            sent_label = \"Positive\"\n",
    "        elif sentiment < -0.25 and sentiment > -0.5:\n",
    "            sent_label = \"Neutral Negative\"\n",
    "        elif sentiment <= -0.5:\n",
    "            sent_label = \"Negative\"\n",
    "\n",
    "        # Appending labels to the dictionary\n",
    "        dictionary[\"Sentiment Label\"].append(sent_label)\n",
    "        dictionary[\"Sentiment Score\"].append(sentiment)\n",
    "        dictionary[\"Subjectivity Score\"].append(subjectivity)\n",
    "        dictionary[\"Text\"].append(text)\n",
    "\n",
    "        positive_words = []\n",
    "        negative_words = []\n",
    "\n",
    "        # Creating a list of positive and negative words\n",
    "        for x in doc._.blob.sentiment_assessments.assessments:\n",
    "            if x[1] > 0:\n",
    "                positive_words.append(x[0][0])\n",
    "            elif x[1] < 0:\n",
    "                negative_words.append(x[0][0])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        dictionary[\"Positive Words\"].append(', '.join(set(positive_words)))\n",
    "        dictionary[\"Negative Words\"].append(', '.join(set(negative_words)))\n",
    "\n",
    "    # Hits here if there was a scraping error\n",
    "    else:\n",
    "        dictionary[\"URL\"].append(url)\n",
    "        dictionary[\"Article Title\"].append(article_title)\n",
    "        dictionary[\"Sentiment Label\"].append(text)\n",
    "        dictionary[\"Sentiment Score\"].append(0.0)\n",
    "        dictionary[\"Subjectivity Score\"].append(0.0)\n",
    "        dictionary[\"Text\"].append(text)\n",
    "\n",
    "        positive_words = []\n",
    "        negative_words = []\n",
    "\n",
    "        dictionary[\"Positive Words\"].append(', '.join(set(positive_words)))\n",
    "        dictionary[\"Negative Words\"].append(', '.join(set(negative_words)))\n",
    "\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3cf0fc",
   "metadata": {},
   "source": [
    "# Topic Level Sentiment\n",
    "The following code blocks are used to preform the sentiment analysis based on the topic word(s) of an article. It will perform sentiment on the sentences that contain the topic(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6a58880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_words_dict(ldamodel):\n",
    "    \"\"\"\n",
    "    Taking in the LDA model, this returns a dictionary of topics, where the \n",
    "    values for each topic is a list of its top 10 associated words.\n",
    "    \"\"\"\n",
    "    my_dict = {i: [token for token, \n",
    "                   score in ldamodel.show_topic(i, topn=10)] \\\n",
    "                    for i in range(0, ldamodel.num_topics)}\n",
    "\n",
    "    return my_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "25ad14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(doc):\n",
    "    \"\"\"\n",
    "    Taking in a Spacy doc object, returns all sentences of the doc as a list.\n",
    "    \"\"\"\n",
    "    return doc.sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "47eff2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_sentiment_from_doc(doc):\n",
    "    \"\"\"\n",
    "    Taking in a Spacy doc object, return a list of tuples of all sentences \n",
    "    in the doc and their associated sentiment value (for that sentence) in the \n",
    "    form (sentence, sentiment value).\n",
    "    \"\"\"\n",
    "    sentences = get_sentences(doc)\n",
    "    tuple_list = []\n",
    "    for sentence in sentences:\n",
    "        sent_doc = nlp(sentence.text)\n",
    "        # List of tuples of form [(text, sentiment)]\n",
    "        tuple_list.append((sentence.text,sent_doc._.blob.polarity))\n",
    "    return tuple_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bae6a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_sentiment_on_topics(doc, topic_list):\n",
    "    \"\"\"\n",
    "    Taking in a Spacy doc object and a generated topic word dictionary \n",
    "    (from create_topic_words_dict()), this iterates through every topic in\n",
    "    the dictionary and goes through every word of every topic. For each word, \n",
    "    it finds all occurences of the word in the doc and the local sentiment \n",
    "    scores for that word. Then, when we have all sentiment values for all words \n",
    "    in a topic, averages out the sentiment score for that topic and adds it to \n",
    "    a dictionary. Finally, we return the dictionary of all topics and their \n",
    "    averaged sentiment scores for each of those topics for that singlearticle.\n",
    "    \"\"\"\n",
    "    # Get all sentences and their sentiment\n",
    "    sentence_sentiment_list = sentence_sentiment_from_doc(doc)\n",
    "    score_list = []\n",
    "    return_dict = {}\n",
    "\n",
    "    # For every topic\n",
    "    for key in topic_list:\n",
    "        # For every word in that topic\n",
    "        for word in topic_list[key]:\n",
    "            for sentence, sentiment in sentence_sentiment_list:\n",
    "                # If the word is in that sentence we add the sentiment value\n",
    "                if sentence.find(word) != -1:\n",
    "                    score_list.append(sentiment)\n",
    "        if not score_list:\n",
    "            return_dict[key] = 0\n",
    "        else:\n",
    "            # Average of all sentence sentiments for topic\n",
    "            return_dict[key] = sum(score_list) / len(score_list)\n",
    "\n",
    "    return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6340b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_sentence_sentiment_analysis(df, lda_model):\n",
    "    \"\"\"\n",
    "    Takes in our main sentiment analysis dataframe, our LDA model, and the LDA \n",
    "    model corpus. This uses all of the functions above it to generate a\n",
    "    dataframe of all articles (rows) and all topics (columns) where the cell \n",
    "    corresponds to the localized sentiment value for that topic on that article.\n",
    "\n",
    "    This is done by gathering each document text and making it a Spacy doc \n",
    "    object, then performing our localized sentiment analysis function on that \n",
    "    article and adding that to a larger dictionary of all articles. This will \n",
    "    return a dictionary of dictionaries, where each key is an article URL and \n",
    "    the value is a dictionary of all topics and their sentiment values\n",
    "    for that article.\n",
    "    \"\"\"\n",
    "    topic_sent_dic = {}\n",
    "    # List of topics and their words\n",
    "    topic_list = create_topic_words_dict(lda_model)\n",
    "\n",
    "    # For every article\n",
    "    for x in range(len(df[\"URL\"])):\n",
    "        page_text = df.iloc[x][\"Text\"]\n",
    "        # Gather page text and transform into doc object\n",
    "        tempdoc = nlp(page_text)\n",
    "\n",
    "        # Dictionary of all topics and their average sentiment for the article\n",
    "        temp = sentence_sentiment_on_topics(tempdoc,topic_list)\n",
    "        topic_sent_dic[x] = temp # Append sentiment dict\n",
    "\n",
    "    return topic_sent_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286ecf4",
   "metadata": {},
   "source": [
    "# DEPRECATED FUNCTIONS\n",
    "\n",
    "Functions are no longer in use but are kept here in case of a need for them in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dfdc2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_sentiment_per_doc(word, text):\n",
    "    \"\"\"\n",
    "    DEPRECATED FUNCTION (not in use). Takes in a word (string) and a Spacy doc\n",
    "    object, and get the localized sentiment value for that word in the document. \n",
    "    Similar to our sentence_sentiment_on_topics() function but only does it for\n",
    "    one word.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    # Get all sentences and their sentiment\n",
    "    sentence_sentiment_list = sentence_sentiment_from_doc(doc)\n",
    "    word_score = 0\n",
    "    total_appearences = 0\n",
    "\n",
    "    for sentence, sentiment in sentence_sentiment_list:\n",
    "        # If the word is in that sentence we add the sentiment value\n",
    "        if sentence.find(word) != -1:\n",
    "            word_score += sentiment \n",
    "            total_appearences += 1\n",
    "\n",
    "    if total_appearences == 0:\n",
    "        return None\n",
    "\n",
    "    word_sentiment = word_score / total_appearences\n",
    "    return word_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1e896fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_sentiment_per_doc(topics, text):\n",
    "    \"\"\"\n",
    "    DEPRECATED FUNCTION (not in use). Takes in a list of relevant topics and \n",
    "    the text of an article, and returns the local sentiment score for the topic\n",
    "    on that article (using a weighted score).\n",
    "    \"\"\"\n",
    "    topic_sentiment_df = []\n",
    "\n",
    "    for i, topic_tuple in enumerate(topics):\n",
    "        topic_id, topic = topic_tuple\n",
    "        # For each topic\n",
    "        weighted_topic_sentiment = 0\n",
    "        for word, score in topic:\n",
    "            # For each word in a topic\n",
    "            # Multiply the relavence by the sentiment to get a weighted sentiment\n",
    "            word_sentiment = word_sentiment_per_doc(word, text)\n",
    "            if word_sentiment is not None:\n",
    "                weighted_word_sentiment = score * word_sentiment\n",
    "                weighted_topic_sentiment += weighted_word_sentiment\n",
    "        topic_sentiment_df.append((topic_id, weighted_topic_sentiment))\n",
    "\n",
    "    return topic_sentiment_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "434hw",
   "language": "python",
   "name": "434hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
