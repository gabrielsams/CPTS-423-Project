{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model Generation (Building Blocks of Main Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    \"\"\"Loads a file for reading with json, and returns the open file.\"\"\"\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def write_data(file, data):\n",
    "    \"\"\"Takes in a file and data to write, and writes the data onto the \n",
    "    file with json.\"\"\"\n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"AV\"]):\n",
    "    \"\"\"Taking in a list of article texts, lemmatizes the words and returns \n",
    "    a new text list of all lemmatized text.\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    text_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        text_out.append(final)\n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(input_text):\n",
    "    \"\"\"\n",
    "    Taking in an article text, splits the text by words and removes all stopwords\n",
    "    (from a stopwords file) and also removes leading and trailing whitespace.\n",
    "    Returns the cleaned text.\n",
    "    \"\"\"\n",
    "    sentences_ted = []\n",
    "\n",
    "    # Use regular expression to split the text into words\n",
    "    sentences_ted = re.findall(r'\\b\\w+\\b', input_text)\n",
    "    sentences_ted = [token for token in sentences_ted if not token.isdigit()]\n",
    "\n",
    "\n",
    "    # Load stopwords from a file into a set\n",
    "    stoplist = set()\n",
    "    with open('stopwords.txt') as openfileobject: \n",
    "        for line in openfileobject:\n",
    "            # Use strip() to remove leading/trailing whitespace\n",
    "            stoplist.add(line.strip())\n",
    "\n",
    "    cleaned_text = \" \".join(word for word in sentences_ted if word not in stoplist)\n",
    "\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    \"\"\"\n",
    "    Taking in a list of text, preprocesses and returns \n",
    "    the text as tokenized words.\n",
    "    \"\"\"\n",
    "    final = [gensim.utils.simple_preprocess(text, deacc=True) for text in texts]\n",
    "\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, tokenizedData, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Takes in our word dictionary, the article corpus, the tokenized words, \n",
    "    and three settings limit (max topics), start (min topics), and\n",
    "    step (iteration value). This creates a list of LDA models with varying\n",
    "    numbers of topics which start at start and end at limit, incrementing by step.\n",
    "    It also creates a matching list of those models' coherence values, which\n",
    "    correspond to how well the model describes our data.\n",
    "    Returns the list of models and their corresponding coherence values.\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                                num_topics=num_topics, \n",
    "                                                id2word=dictionary, \n",
    "                                                passes=10)\n",
    "\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, \n",
    "                                        dictionary=dictionary, \n",
    "                                        texts=tokenizedData, \n",
    "                                        coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model Generation (Main Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lda_model(topic_limit, topic_start, topic_step):\n",
    "    \"\"\"\n",
    "    Takes in the generated sentiment analysis dataframe, and the settings \n",
    "    topic_limit (max), topic_start (min), and topic_step(increment). This walks\n",
    "    through the entire process of creating a list of LDA_models and returns the \n",
    "    model with the highest coherence, as well as generating a graph showing which\n",
    "    number of topics had what coherence score.\n",
    "\n",
    "    First we gather all of the article texts from the dataframe, lemmatize it and\n",
    "    remove all stopwords. We tokenize each text and use the tokenized text to\n",
    "    generate a word dictionary. We make the corpus for our articles, and then\n",
    "    we generate our LDA models.\n",
    "\n",
    "    It creates a list of LDA models and their corresponding coherence values\n",
    "    based on the settings we input. It generates a visual of all models and their\n",
    "    coherence scores based on the number of topics they had. Finally, returns \n",
    "    the most coherent LDA model and the text corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pre-Process text grabbing\n",
    "    raw_data = []\n",
    "    # Get all texts in this data structure\n",
    "    for index, row in df.iterrows():\n",
    "        raw_data.append(row['Text'])\n",
    "\n",
    "    # Lemmatize the texts\n",
    "    lemmatized_data = lemmatization(raw_data)\n",
    "\n",
    "    # Removing Stop Words\n",
    "    filtered_data = [preprocess_article(x) for x in lemmatized_data]\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_data = gen_words(filtered_data)\n",
    "\n",
    "    # Create text dictionary\n",
    "    id2word = corpora.Dictionary(tokenized_data)\n",
    "    id2word.filter_extremes(no_below=0.1, no_above=0.9)\n",
    "\n",
    "    # Create corpus\n",
    "    corpus = [id2word.doc2bow(text) for text in tokenized_data]\n",
    "\n",
    "    # Topic modeling using input values\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=id2word,\n",
    "                                                            corpus=corpus,\n",
    "                                                            tokenizedData=tokenized_data,\n",
    "                                                            limit=topic_limit,\n",
    "                                                            start=topic_start,\n",
    "                                                            step=topic_step)\n",
    "\n",
    "    #Coherence score visualization\n",
    "    x = range(topic_start, topic_limit, topic_step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    # Find the model with max coherence\n",
    "    max_coherence_index = coherence_values.index(max(coherence_values))\n",
    "    lda_model = model_list[max_coherence_index]\n",
    "\n",
    "    return lda_model, corpus # This is our LDA model object that we will work with\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "434hw",
   "language": "python",
   "name": "434hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
